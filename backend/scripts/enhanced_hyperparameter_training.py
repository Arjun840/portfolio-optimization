#!/usr/bin/env python3
"""
Enhanced ML Training with Comprehensive Hyperparameter Tuning

This script implements the complete ML training pipeline with:
- GridSearchCV hyperparameter tuning for all models
- TimeSeriesSplit cross-validation (respects temporal order)
- Proper 80/20 temporal train/test split (no forward-looking bias)
- Multiple evaluation metrics (MSE, MAE, RÂ², Sharpe ratio)
- Enhanced feature engineering
- Model comparison and selection
- Comprehensive performance analysis

Models with hyperparameter grids:
- Ridge/Lasso Regression (alpha tuning)
- Random Forest (depth, estimators, features)
- Gradient Boosting (learning rate, depth, estimators)
- Support Vector Regression (C, gamma, kernel)
- XGBoost (if available)
"""

import pandas as pd
import numpy as np
import pickle
import json
import warnings
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# ML Libraries
from sklearn.model_selection import (
    TimeSeriesSplit, GridSearchCV, cross_val_score, 
    ParameterGrid
)
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer
from sklearn.pipeline import Pipeline

# Try XGBoost
try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False
    print("âš ï¸  XGBoost not available - will skip XGBoost models")

warnings.filterwarnings('ignore')

class EnhancedHyperparameterTrainer:
    """Enhanced ML trainer with comprehensive hyperparameter tuning."""
    
    def __init__(self, data_dir: str = "data"):
        """Initialize the enhanced trainer."""
        self.data_dir = data_dir
        self.prices = None
        self.returns = None
        self.results = {}
        
        # Prediction horizons
        self.horizons = {'1M': 21, '3M': 63, '6M': 126}
        
        # CV configuration
        self.n_splits = 5
        self.test_size = 0.2
        
        # Model configurations with comprehensive hyperparameter grids
        self.model_configs = self._setup_model_configs()
        
        print(f"ðŸ”¬ Enhanced Hyperparameter Trainer initialized")
        print(f"   â€¢ Models with tuning: {len(self.model_configs)}")
        print(f"   â€¢ CV splits: {self.n_splits}")
        print(f"   â€¢ Test holdout: {self.test_size:.0%}")
    
    def _setup_model_configs(self) -> Dict:
        """Setup models with comprehensive hyperparameter grids."""
        configs = {
            'ridge': {
                'model': Ridge(random_state=42),
                'param_grid': {
                    'alpha': [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
                },
                'scoring': 'neg_mean_squared_error'
            },
            'lasso': {
                'model': Lasso(random_state=42, max_iter=2000),
                'param_grid': {
                    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
                },
                'scoring': 'neg_mean_squared_error'
            },
            'elastic_net': {
                'model': ElasticNet(random_state=42, max_iter=2000),
                'param_grid': {
                    'alpha': [0.01, 0.1, 1.0, 10.0],
                    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]
                },
                'scoring': 'neg_mean_squared_error'
            },
            'random_forest': {
                'model': RandomForestRegressor(random_state=42, n_jobs=-1),
                'param_grid': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [5, 10, 15, None],
                    'min_samples_split': [2, 5, 10],
                    'min_samples_leaf': [1, 2, 4],
                    'max_features': ['sqrt', 'log2', 0.5]
                },
                'scoring': 'neg_mean_squared_error'
            },
            'gradient_boosting': {
                'model': GradientBoostingRegressor(random_state=42),
                'param_grid': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 5, 7],
                    'learning_rate': [0.01, 0.05, 0.1, 0.2],
                    'subsample': [0.8, 0.9, 1.0],
                    'min_samples_split': [2, 5, 10]
                },
                'scoring': 'neg_mean_squared_error'
            },
            'svr': {
                'model': SVR(),
                'param_grid': {
                    'C': [0.1, 1.0, 10.0, 100.0],
                    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1.0],
                    'kernel': ['rbf', 'linear', 'poly'],
                    'epsilon': [0.01, 0.1, 0.2]
                },
                'scoring': 'neg_mean_squared_error'
            }
        }
        
        # Add XGBoost if available
        if XGBOOST_AVAILABLE:
            configs['xgboost'] = {
                'model': xgb.XGBRegressor(random_state=42, n_jobs=-1, verbosity=0),
                'param_grid': {
                    'n_estimators': [50, 100, 200],
                    'max_depth': [3, 5, 7],
                    'learning_rate': [0.01, 0.05, 0.1, 0.2],
                    'subsample': [0.8, 0.9, 1.0],
                    'colsample_bytree': [0.8, 0.9, 1.0],
                    'reg_alpha': [0, 0.1, 1.0],
                    'reg_lambda': [1, 1.5, 2.0]
                },
                'scoring': 'neg_mean_squared_error'
            }
        
        return configs
    
    def load_data(self) -> bool:
        """Load price and returns data."""
        try:
            print("ðŸ“Š Loading Yahoo Finance data...")
            
            # Load from pickle (faster) or CSV
            price_file = Path(self.data_dir) / "price_matrix_latest.pkl"
            returns_file = Path(self.data_dir) / "returns_matrix_latest.pkl"
            
            if price_file.exists() and returns_file.exists():
                self.prices = pd.read_pickle(price_file)
                self.returns = pd.read_pickle(returns_file)
            else:
                price_file = Path(self.data_dir) / "price_matrix_latest.csv"
                returns_file = Path(self.data_dir) / "returns_matrix_latest.csv"
                self.prices = pd.read_csv(price_file, index_col=0, parse_dates=True)
                self.returns = pd.read_csv(returns_file, index_col=0, parse_dates=True)
            
            print(f"âœ… Data loaded:")
            print(f"   â€¢ Price matrix: {self.prices.shape}")
            print(f"   â€¢ Returns matrix: {self.returns.shape}")
            print(f"   â€¢ Date range: {self.prices.index[0].date()} to {self.prices.index[-1].date()}")
            print(f"   â€¢ Total assets: {len(self.prices.columns)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load data: {e}")
            return False
    
    def create_enhanced_features(self, asset: str, horizon: str) -> Tuple[pd.DataFrame, pd.Series]:
        """Create comprehensive features for ML training."""
        horizon_days = self.horizons[horizon]
        asset_returns = self.returns[asset].dropna()
        asset_prices = self.prices[asset].dropna()
        
        # Market proxy (SPY or first asset)
        market_asset = 'SPY' if 'SPY' in self.returns.columns else self.returns.columns[0]
        market_returns = self.returns[market_asset].dropna()
        
        feature_data = []
        target_data = []
        dates = []
        
        # Need sufficient lookback for features
        min_lookback = 126  # 6 months
        start_idx = max(min_lookback, horizon_days)
        
        for i in range(start_idx, len(asset_returns) - horizon_days):
            date = asset_returns.index[i]
            features = {}
            
            # === BASIC RETURN FEATURES ===
            # Multiple timeframes
            for days in [5, 10, 21, 63, 126]:
                if i >= days:
                    ret = asset_returns.iloc[i-days:i].sum()
                    vol = asset_returns.iloc[i-days:i].std() * np.sqrt(252)
                    features[f'return_{days}d'] = ret
                    features[f'volatility_{days}d'] = vol
                    
                    # Sharpe ratio
                    if vol > 0:
                        features[f'sharpe_{days}d'] = (ret * 252) / vol
                    else:
                        features[f'sharpe_{days}d'] = 0.0
            
            # === MOMENTUM FEATURES ===
            if i >= 21:
                features['momentum_1m'] = asset_returns.iloc[i-21:i].sum()
            if i >= 63:
                features['momentum_3m'] = asset_returns.iloc[i-63:i].sum()
                features['momentum_6m'] = asset_returns.iloc[i-126:i].sum() if i >= 126 else 0
            
            # === MEAN REVERSION FEATURES ===
            current_price = asset_prices.iloc[i]
            if i >= 21:
                sma21 = asset_prices.iloc[i-21:i].mean()
                features['price_sma21_ratio'] = current_price / sma21 - 1
            if i >= 63:
                sma63 = asset_prices.iloc[i-63:i].mean()
                features['price_sma63_ratio'] = current_price / sma63 - 1
            
            # === TECHNICAL INDICATORS ===
            if i >= 14:
                # RSI approximation
                gains = asset_returns.iloc[i-14:i].clip(lower=0).sum()
                losses = (-asset_returns.iloc[i-14:i].clip(upper=0)).sum()
                if losses > 0:
                    rs = gains / losses
                    features['rsi_14'] = 100 - (100 / (1 + rs))
                else:
                    features['rsi_14'] = 100
            
            # === MARKET RELATIVE FEATURES ===
            if i >= 21:
                asset_ret = asset_returns.iloc[i-21:i].sum()
                market_ret = market_returns.iloc[i-21:i].sum()
                features['excess_return_21d'] = asset_ret - market_ret
                
                # Beta estimation
                asset_rets_21 = asset_returns.iloc[i-21:i]
                market_rets_21 = market_returns.iloc[i-21:i]
                if len(asset_rets_21) == len(market_rets_21) and market_rets_21.std() > 0:
                    corr = asset_rets_21.corr(market_rets_21)
                    beta = corr * (asset_rets_21.std() / market_rets_21.std())
                    features['beta_21d'] = beta if not pd.isna(beta) else 1.0
                else:
                    features['beta_21d'] = 1.0
            
            # === LAGGED RETURNS ===
            for lag in [1, 2, 3, 5, 10]:
                if i >= lag:
                    features[f'return_lag_{lag}'] = asset_returns.iloc[i-lag]
            
            # === VOLATILITY CLUSTERING ===
            if i >= 21:
                recent_vol = asset_returns.iloc[i-5:i].std()
                hist_vol = asset_returns.iloc[i-21:i-5].std()
                if hist_vol > 0:
                    features['vol_ratio'] = recent_vol / hist_vol
                else:
                    features['vol_ratio'] = 1.0
            
            # === CALENDAR EFFECTS ===
            features['month'] = date.month
            features['quarter'] = date.quarter
            features['day_of_week'] = date.dayofweek
            features['is_month_end'] = 1 if date.day >= 25 else 0
            features['is_quarter_end'] = 1 if date.month in [3, 6, 9, 12] and date.day >= 25 else 0
            
            # === MARKET ENVIRONMENT ===
            if i >= 63:
                market_vol = market_returns.iloc[i-63:i].std() * np.sqrt(252)
                market_ret = market_returns.iloc[i-63:i].sum()
                features['market_volatility_3m'] = market_vol
                features['market_return_3m'] = market_ret
            
            # === TARGET ===
            future_returns = asset_returns.iloc[i:i+horizon_days]
            if len(future_returns) == horizon_days:
                target = future_returns.sum()
                
                # Only include valid samples
                if (all(np.isfinite(v) for v in features.values()) and 
                    np.isfinite(target)):
                    feature_data.append(features)
                    target_data.append(target)
                    dates.append(date)
        
        if not feature_data:
            raise ValueError(f"No valid features created for {asset}")
        
        features_df = pd.DataFrame(feature_data, index=dates)
        targets_series = pd.Series(target_data, index=dates, name=f'target_{horizon}')
        
        print(f"   ðŸ“Š Created {len(features_df)} samples with {len(features_df.columns)} features")
        return features_df, targets_series
    
    def perform_hyperparameter_tuning(self, model_name: str, config: Dict, 
                                    X_train: np.ndarray, y_train: np.ndarray) -> Dict:
        """Perform comprehensive hyperparameter tuning with TimeSeriesSplit."""
        print(f"   ðŸ” Hyperparameter tuning for {model_name}...")
        
        # Time series cross-validation
        tscv = TimeSeriesSplit(n_splits=self.n_splits)
        
        # Create pipeline
        pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('model', config['model'])
        ])
        
        # Prepare parameter grid
        param_grid = {}
        for param, values in config['param_grid'].items():
            param_grid[f'model__{param}'] = values
        
        # Calculate total combinations
        total_combinations = 1
        for values in config['param_grid'].values():
            total_combinations *= len(values)
        
        print(f"      â€¢ Parameter combinations: {total_combinations}")
        print(f"      â€¢ Total CV fits: {total_combinations * self.n_splits}")
        
        try:
            # GridSearchCV with time series splits
            grid_search = GridSearchCV(
                pipeline,
                param_grid,
                cv=tscv,
                scoring=config['scoring'],
                n_jobs=-1,
                verbose=0,
                return_train_score=True,
                error_score='raise'
            )
            
            grid_search.fit(X_train, y_train)
            
            # Extract results
            results = {
                'best_estimator': grid_search.best_estimator_,
                'best_params': grid_search.best_params_,
                'best_score': grid_search.best_score_,
                'cv_results': {
                    'mean_test_score': grid_search.cv_results_['mean_test_score'],
                    'std_test_score': grid_search.cv_results_['std_test_score'],
                    'mean_train_score': grid_search.cv_results_['mean_train_score'],
                    'std_train_score': grid_search.cv_results_['std_train_score'],
                    'params': grid_search.cv_results_['params']
                }
            }
            
            print(f"      âœ… Best CV score: {grid_search.best_score_:.4f}")
            print(f"      âœ… Best params: {grid_search.best_params_}")
            
            return results
            
        except Exception as e:
            print(f"      âŒ GridSearch failed for {model_name}: {e}")
            # Fallback: use default model
            pipeline.fit(X_train, y_train)
            return {
                'best_estimator': pipeline,
                'best_params': {},
                'best_score': None,
                'cv_results': None,
                'error': str(e)
            }
    
    def evaluate_model(self, model, X_train: np.ndarray, y_train: np.ndarray,
                      X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """Comprehensive model evaluation with financial metrics."""
        # Predictions
        train_pred = model.predict(X_train)
        test_pred = model.predict(X_test)
        
        # Standard metrics
        metrics = {
            'train_mse': mean_squared_error(y_train, train_pred),
            'test_mse': mean_squared_error(y_test, test_pred),
            'train_mae': mean_absolute_error(y_train, train_pred),
            'test_mae': mean_absolute_error(y_test, test_pred),
            'train_r2': r2_score(y_train, train_pred),
            'test_r2': r2_score(y_test, test_pred)
        }
        
        # RMSE
        metrics['train_rmse'] = np.sqrt(metrics['train_mse'])
        metrics['test_rmse'] = np.sqrt(metrics['test_mse'])
        
        # Overfitting measure
        metrics['overfitting'] = metrics['train_r2'] - metrics['test_r2']
        
        # Financial metrics
        if len(test_pred) > 1:
            # Hit ratio (directional accuracy)
            actual_sign = np.sign(y_test)
            pred_sign = np.sign(test_pred)
            metrics['hit_ratio'] = np.mean(actual_sign == pred_sign)
            
            # Information coefficient
            from scipy.stats import spearmanr
            ic, ic_pval = spearmanr(y_test, test_pred)
            metrics['information_coefficient'] = ic if not np.isnan(ic) else 0
            metrics['ic_pvalue'] = ic_pval if not np.isnan(ic_pval) else 1
            
            # Prediction sharpe
            if np.std(test_pred) > 0:
                metrics['prediction_sharpe'] = np.mean(test_pred) / np.std(test_pred) * np.sqrt(252)
            else:
                metrics['prediction_sharpe'] = 0
        
        return metrics
    
    def train_asset_horizon(self, asset: str, horizon: str) -> Dict:
        """Train all models for specific asset and horizon with hyperparameter tuning."""
        print(f"\nðŸŽ¯ Training {asset} - {horizon} horizon with hyperparameter tuning")
        print("=" * 60)
        
        try:
            # Create features
            features_df, targets = self.create_enhanced_features(asset, horizon)
            
            if len(features_df) < 100:
                print(f"âš ï¸  Insufficient data: {len(features_df)} samples")
                return {}
            
            # Clean data
            features_df = features_df.replace([np.inf, -np.inf], np.nan)
            features_df = features_df.fillna(features_df.median())
            
            # Remove invalid targets
            valid_mask = ~targets.isna()
            features_df = features_df[valid_mask]
            targets = targets[valid_mask]
            
            # Temporal split (80% train, 20% test)
            split_idx = int(len(features_df) * (1 - self.test_size))
            
            X_train = features_df.iloc[:split_idx].values
            X_test = features_df.iloc[split_idx:].values
            y_train = targets.iloc[:split_idx].values
            y_test = targets.iloc[split_idx:].values
            
            print(f"   ðŸ“Š Data split: {len(X_train)} train, {len(X_test)} test")
            print(f"   ðŸ“… Train: {features_df.index[0].date()} to {features_df.index[split_idx-1].date()}")
            print(f"   ðŸ“… Test: {features_df.index[split_idx].date()} to {features_df.index[-1].date()}")
            
            results = {
                'models': {},
                'tuning_results': {},
                'performance': {},
                'data_info': {
                    'train_size': len(X_train),
                    'test_size': len(X_test),
                    'n_features': X_train.shape[1],
                    'train_period': (features_df.index[0], features_df.index[split_idx-1]),
                    'test_period': (features_df.index[split_idx], features_df.index[-1])
                }
            }
            
            # Train each model with hyperparameter tuning
            for model_name, config in self.model_configs.items():
                print(f"\n   ðŸ¤– Training {model_name}...")
                
                try:
                    # Hyperparameter tuning
                    tuning_result = self.perform_hyperparameter_tuning(
                        model_name, config, X_train, y_train
                    )
                    
                    # Get best model
                    best_model = tuning_result['best_estimator']
                    
                    # Evaluate model
                    performance = self.evaluate_model(
                        best_model, X_train, y_train, X_test, y_test
                    )
                    
                    # Store results
                    results['models'][model_name] = best_model
                    results['tuning_results'][model_name] = {
                        'best_params': tuning_result['best_params'],
                        'best_cv_score': tuning_result['best_score'],
                        'cv_std': np.std(tuning_result['cv_results']['mean_test_score']) if tuning_result['cv_results'] else None
                    }
                    results['performance'][model_name] = performance
                    
                    # Print key metrics
                    print(f"      ðŸ“ˆ Test RÂ²: {performance['test_r2']:.3f}")
                    print(f"      ðŸ“‰ Test RMSE: {performance['test_rmse']:.4f}")
                    print(f"      ðŸŽ¯ Hit Ratio: {performance['hit_ratio']:.3f}")
                    print(f"      ðŸ“Š Info Coeff: {performance['information_coefficient']:.3f}")
                    
                except Exception as e:
                    print(f"      âŒ Error training {model_name}: {e}")
                    continue
            
            # Find best model
            if results['performance']:
                best_model_name = max(
                    results['performance'].items(),
                    key=lambda x: x[1]['test_r2']
                )[0]
                results['best_model'] = best_model_name
                best_r2 = results['performance'][best_model_name]['test_r2']
                print(f"\n   ðŸ† Best model: {best_model_name} (RÂ² = {best_r2:.3f})")
            
            return results
            
        except Exception as e:
            print(f"âŒ Error training {asset}-{horizon}: {e}")
            return {}
    
    def train_all_models(self, assets: List[str] = None, horizons: List[str] = None):
        """Train all models with hyperparameter tuning."""
        if assets is None:
            # Representative sample
            sample_assets = ['AAPL', 'MSFT', 'GOOGL', 'JPM', 'JNJ', 'SPY', 'QQQ', 'TLT', 'GLD']
            assets = [a for a in sample_assets if a in self.returns.columns]
        
        if horizons is None:
            horizons = ['1M', '3M']
        
        print(f"\nðŸš€ ENHANCED HYPERPARAMETER TUNING TRAINING")
        print("=" * 80)
        print(f"Configuration:")
        print(f"   â€¢ Assets: {len(assets)} ({', '.join(assets)})")
        print(f"   â€¢ Horizons: {horizons}")
        print(f"   â€¢ Models: {list(self.model_configs.keys())}")
        print(f"   â€¢ CV method: TimeSeriesSplit ({self.n_splits} folds)")
        print(f"   â€¢ Test holdout: {self.test_size:.0%}")
        
        for asset in assets:
            if asset not in self.returns.columns:
                print(f"âš ï¸  Skipping {asset} - not in data")
                continue
            
            self.results[asset] = {}
            
            for horizon in horizons:
                result = self.train_asset_horizon(asset, horizon)
                self.results[asset][horizon] = result
        
        print(f"\nâœ… Enhanced hyperparameter tuning complete!")
        self._print_summary()
    
    def _print_summary(self):
        """Print comprehensive training summary."""
        print(f"\nðŸ“Š HYPERPARAMETER TUNING SUMMARY")
        print("=" * 80)
        
        # Collect results
        all_results = []
        for asset, horizons in self.results.items():
            for horizon, data in horizons.items():
                if 'performance' in data:
                    for model, perf in data['performance'].items():
                        all_results.append({
                            'Asset': asset,
                            'Horizon': horizon,
                            'Model': model,
                            'Test_R2': perf['test_r2'],
                            'Test_RMSE': perf['test_rmse'],
                            'Hit_Ratio': perf['hit_ratio'],
                            'Info_Coeff': perf['information_coefficient'],
                            'CV_Score': data['tuning_results'][model]['best_cv_score']
                        })
        
        if not all_results:
            print("No results to summarize.")
            return
        
        df = pd.DataFrame(all_results)
        
        # Top performers
        print("ðŸ† TOP PERFORMERS BY TEST RÂ²:")
        top_r2 = df.nlargest(10, 'Test_R2')
        for _, row in top_r2.iterrows():
            print(f"   {row['Asset']:5s} {row['Horizon']:2s} {row['Model']:15s}: RÂ²={row['Test_R2']:6.3f}, Hit={row['Hit_Ratio']:5.3f}")
        
        # Model comparison
        print(f"\nðŸ“ˆ AVERAGE PERFORMANCE BY MODEL:")
        model_stats = df.groupby('Model').agg({
            'Test_R2': ['mean', 'std', 'count'],
            'Hit_Ratio': ['mean', 'std'],
            'Info_Coeff': ['mean', 'std']
        }).round(3)
        
        for model in model_stats.index:
            r2_mean = model_stats.loc[model, ('Test_R2', 'mean')]
            r2_std = model_stats.loc[model, ('Test_R2', 'std')]
            hit_mean = model_stats.loc[model, ('Hit_Ratio', 'mean')]
            count = model_stats.loc[model, ('Test_R2', 'count')]
            print(f"   {model:15s}: RÂ²={r2_mean:6.3f}Â±{r2_std:5.3f}, Hit={hit_mean:5.3f} (n={count:2.0f})")
    
    def save_results(self):
        """Save comprehensive results."""
        print(f"\nðŸ’¾ Saving enhanced training results...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_dir = Path(self.data_dir) / 'enhanced_hyperparameter_results'
        results_dir.mkdir(exist_ok=True)
        
        # Performance summary
        all_results = []
        for asset, horizons in self.results.items():
            for horizon, data in horizons.items():
                if 'performance' in data:
                    for model, perf in data['performance'].items():
                        tuning_info = data['tuning_results'][model]
                        all_results.append({
                            'Asset': asset,
                            'Horizon': horizon,
                            'Model': model,
                            'Test_R2': perf['test_r2'],
                            'Test_RMSE': perf['test_rmse'],
                            'Test_MAE': perf['test_mae'],
                            'Hit_Ratio': perf['hit_ratio'],
                            'Information_Coefficient': perf['information_coefficient'],
                            'CV_Score': tuning_info['best_cv_score'],
                            'CV_Std': tuning_info['cv_std'],
                            'Best_Params': str(tuning_info['best_params'])
                        })
        
        if all_results:
            results_df = pd.DataFrame(all_results)
            results_df.to_csv(results_dir / f'hyperparameter_results_{timestamp}.csv', index=False)
            results_df.to_csv(results_dir / 'hyperparameter_results_latest.csv', index=False)
        
        # Detailed results (JSON)
        serializable_results = {}
        for asset, horizons in self.results.items():
            serializable_results[asset] = {}
            for horizon, data in horizons.items():
                if data:
                    serializable_results[asset][horizon] = {
                        'performance': data.get('performance', {}),
                        'tuning_results': data.get('tuning_results', {}),
                        'data_info': data.get('data_info', {}),
                        'best_model': data.get('best_model', None)
                    }
        
        with open(results_dir / f'detailed_results_{timestamp}.json', 'w') as f:
            json.dump(serializable_results, f, indent=2, default=str)
        
        print(f"âœ… Results saved to {results_dir}/")
        print(f"   â€¢ Performance CSV: hyperparameter_results_latest.csv")
        print(f"   â€¢ Detailed JSON: detailed_results_{timestamp}.json")
    
    def run_training(self, assets: List[str] = None, horizons: List[str] = None):
        """Run complete enhanced training pipeline."""
        print("ðŸš€ ENHANCED ML TRAINING WITH COMPREHENSIVE HYPERPARAMETER TUNING")
        print("=" * 80)
        print("Features implemented:")
        print("âœ… GridSearchCV with comprehensive parameter grids")
        print("âœ… TimeSeriesSplit cross-validation (respects temporal order)")
        print("âœ… 80/20 temporal train/test split (no forward-looking bias)")
        print("âœ… Multiple evaluation metrics (RÂ², RMSE, Hit Ratio, Information Coefficient)")
        print("âœ… Enhanced feature engineering (30+ features)")
        print("âœ… Financial-specific metrics and validation")
        
        # Load data
        if not self.load_data():
            return
        
        # Train models
        self.train_all_models(assets, horizons)
        
        # Save results
        self.save_results()
        
        print(f"\nðŸŽ‰ ENHANCED HYPERPARAMETER TUNING COMPLETE!")
        print("ðŸ“Š Comprehensive model evaluation with optimal parameters completed!")
        print("ðŸ”® Production-ready models available for portfolio optimization!")


def main():
    """Main function."""
    trainer = EnhancedHyperparameterTrainer()
    
    # Train on representative assets
    assets = ['AAPL', 'MSFT', 'GOOGL', 'JPM', 'JNJ', 'SPY', 'QQQ', 'TLT', 'GLD']
    horizons = ['1M', '3M']
    
    trainer.run_training(assets=assets, horizons=horizons)


if __name__ == "__main__":
    main() 